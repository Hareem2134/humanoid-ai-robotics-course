{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"defaultSidebar":[{"type":"category","label":"Introduction","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/docs/introduction/overview","label":"Overview","docId":"introduction/overview","unlisted":false},{"type":"link","href":"/docs/introduction/learning-outcomes","label":"Learning Outcomes","docId":"introduction/learning-outcomes","unlisted":false},{"type":"link","href":"/docs/introduction/requirements","label":"Hardware and Software Requirements","docId":"introduction/requirements","unlisted":false}]},{"type":"category","label":"Module 1: ROS 2","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/docs/ros-2/introduction-to-ros2","label":"Introduction to ROS 2","docId":"ros-2/introduction-to-ros2","unlisted":false},{"type":"link","href":"/docs/ros-2/ros2-architecture","label":"ROS 2 Architecture","docId":"ros-2/ros2-architecture","unlisted":false},{"type":"link","href":"/docs/ros-2/ros2-nodes-topics-services","label":"ROS 2 Nodes, Topics, and Services","docId":"ros-2/ros2-nodes-topics-services","unlisted":false},{"type":"link","href":"/docs/ros-2/rclpy","label":"Bridging Python Agents to ROS controllers using rclpy","docId":"ros-2/rclpy","unlisted":false},{"type":"link","href":"/docs/ros-2/urdf","label":"Understanding URDF","docId":"ros-2/urdf","unlisted":false}]},{"type":"category","label":"Module 2: Digital Twins","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/docs/gazebo-and-unity/introduction-to-simulation","label":"Introduction to Simulation","docId":"gazebo-and-unity/introduction-to-simulation","unlisted":false},{"type":"link","href":"/docs/gazebo-and-unity/gazebo","label":"Gazebo for Simulation","docId":"gazebo-and-unity/gazebo","unlisted":false},{"type":"link","href":"/docs/gazebo-and-unity/unity","label":"Unity for Photorealistic Simulation","docId":"gazebo-and-unity/unity","unlisted":false},{"type":"link","href":"/docs/gazebo-and-unity/simulating-sensors","label":"Simulating Sensors","docId":"gazebo-and-unity/simulating-sensors","unlisted":false}]},{"type":"category","label":"Module 3: NVIDIA Isaac","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/docs/nvidia-isaac/introduction-to-isaac-sim","label":"Introduction to NVIDIA Isaac Sim","docId":"nvidia-isaac/introduction-to-isaac-sim","unlisted":false},{"type":"link","href":"/docs/nvidia-isaac/visual-slam","label":"Visual SLAM","docId":"nvidia-isaac/visual-slam","unlisted":false},{"type":"link","href":"/docs/nvidia-isaac/nav2","label":"Nav2 for Path Planning","docId":"nvidia-isaac/nav2","unlisted":false},{"type":"link","href":"/docs/nvidia-isaac/reinforcement-learning","label":"Reinforcement Learning for Robotics","docId":"nvidia-isaac/reinforcement-learning","unlisted":false}]},{"type":"category","label":"Module 4: VLA & Humanoid Robotics","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/docs/vla/introduction-to-vla","label":"Introduction to Vision-Language-Action (VLA) Models","docId":"vla/introduction-to-vla","unlisted":false},{"type":"link","href":"/docs/vla/humanoid-kinematics","label":"Humanoid Kinematics and Manipulation","docId":"vla/humanoid-kinematics","unlisted":false},{"type":"link","href":"/docs/vla/conversational-ai","label":"Conversational AI for Robotics","docId":"vla/conversational-ai","unlisted":false},{"type":"link","href":"/docs/vla/capstone-project","label":"Capstone Project","docId":"vla/capstone-project","unlisted":false}]},{"type":"category","label":"Glossary","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/docs/glossary/glossary","label":"Glossary","docId":"glossary/glossary","unlisted":false}]},{"type":"category","label":"Why Physical AI Matters","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/docs/why-physical-ai-matters/introduction","label":"Why Physical AI Matters","docId":"why-physical-ai-matters/introduction","unlisted":false}]},{"type":"category","label":"Learning Outcomes","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/docs/learning-outcomes/introduction","label":"Learning Outcomes","docId":"learning-outcomes/introduction","unlisted":false}]},{"type":"category","label":"Weekly Breakdown","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/docs/weekly-breakdown/introduction","label":"Weekly Breakdown","docId":"weekly-breakdown/introduction","unlisted":false}]},{"type":"category","label":"Assessments","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/docs/assessments/introduction","label":"Assessments","docId":"assessments/introduction","unlisted":false}]},{"type":"category","label":"Hardware Requirements","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/docs/hardware-requirements/introduction","label":"Hardware Requirements","docId":"hardware-requirements/introduction","unlisted":false}]},{"type":"link","href":"/docs/intro","label":"Introduction","docId":"intro","unlisted":false}]},"docs":{"assessments/introduction":{"id":"assessments/introduction","title":"Assessments","description":"*   ROS 2 package development project","sidebar":"defaultSidebar"},"gazebo-and-unity/gazebo":{"id":"gazebo-and-unity/gazebo","title":"Gazebo for Simulation","description":"Gazebo is a powerful and widely used open-source robotics simulator. It allows you to simulate robots in complex and realistic environments, with support for a wide range of sensors and physics engines.","sidebar":"defaultSidebar"},"gazebo-and-unity/introduction-to-simulation":{"id":"gazebo-and-unity/introduction-to-simulation","title":"Introduction to Simulation","description":"Simulation is an indispensable tool in the field of robotics. It allows engineers, developers, and researchers to create a virtual representation of a robot and its environment, enabling them to test and validate their designs, algorithms, and software before deploying them on physical hardware.","sidebar":"defaultSidebar"},"gazebo-and-unity/simulating-sensors":{"id":"gazebo-and-unity/simulating-sensors","title":"Simulating Sensors","description":"Simulating sensors is a critical aspect of robotics simulation. It allows you to test your robot's perception and control algorithms in a virtual environment before deploying them on a physical robot. In this section, we will explore how to simulate some of the most common sensors in robotics.","sidebar":"defaultSidebar"},"gazebo-and-unity/unity":{"id":"gazebo-and-unity/unity","title":"Unity for Photorealistic Simulation","description":"Unity is a powerful and popular game engine that is increasingly being used for robotics simulation. Its advanced rendering capabilities, realistic physics, and intuitive user interface make it an excellent choice for creating photorealistic and interactive simulation environments.","sidebar":"defaultSidebar"},"glossary/glossary":{"id":"glossary/glossary","title":"Glossary","description":"This glossary provides definitions for key terms and acronyms used throughout this book.","sidebar":"defaultSidebar"},"hardware-requirements/introduction":{"id":"hardware-requirements/introduction","title":"Hardware Requirements","description":"This course is technically demanding. It sits at the intersection of three heavy computational loads: Physics Simulation (Isaac Sim/Gazebo), Visual Perception (SLAM/Computer Vision), and Generative AI (LLMs/VLA).","sidebar":"defaultSidebar"},"intro":{"id":"intro","title":"Introduction","description":"Welcome to \"Physical AI & Humanoid Robotics: A Comprehensive Guide\". This book is designed to provide industry practitioners with a comprehensive understanding of humanoid robotics and physical AI.","sidebar":"defaultSidebar"},"introduction/learning-outcomes":{"id":"introduction/learning-outcomes","title":"Learning Outcomes","description":"Upon completion of this book, you will be able to:","sidebar":"defaultSidebar"},"introduction/overview":{"id":"introduction/overview","title":"Overview","description":"Welcome to \"Physical AI & Humanoid Robotics: A Comprehensive Guide\". This book is designed to provide industry practitioners with a comprehensive understanding of humanoid robotics and physical AI.","sidebar":"defaultSidebar"},"introduction/requirements":{"id":"introduction/requirements","title":"Hardware and Software Requirements","description":"To follow along with the examples in this book, you will need the following hardware and software:","sidebar":"defaultSidebar"},"learning-outcomes/introduction":{"id":"learning-outcomes/introduction","title":"Learning Outcomes","description":"*   Understand Physical AI principles and embodied intelligence","sidebar":"defaultSidebar"},"nvidia-isaac/introduction-to-isaac-sim":{"id":"nvidia-isaac/introduction-to-isaac-sim","title":"Introduction to NVIDIA Isaac Sim","description":"NVIDIA Isaac Sim is a powerful robotics simulation and synthetic data generation platform. It is built on top of the NVIDIA Omniverse platform, which is a real-time 3D collaboration and simulation platform. Isaac Sim leverages the power of NVIDIA's RTX GPUs to provide a realistic and physically accurate simulation environment for developing, testing, and training AI-based robots.","sidebar":"defaultSidebar"},"nvidia-isaac/nav2":{"id":"nvidia-isaac/nav2","title":"Nav2 for Path Planning","description":"Nav2 is the second generation of the ROS Navigation Stack. It is a powerful and flexible navigation framework that can be used to control a robot as it moves from one location to another. Nav2 is highly configurable and can be used with a wide variety of robots, including differential drive robots, omnidirectional robots, and even bipedal humanoids.","sidebar":"defaultSidebar"},"nvidia-isaac/reinforcement-learning":{"id":"nvidia-isaac/reinforcement-learning","title":"Reinforcement Learning for Robotics","description":"Reinforcement Learning (RL) is a powerful machine learning paradigm that is well-suited for robotics. In RL, an agent learns to achieve a goal by interacting with an environment. The agent receives a reward for each action it takes, and its goal is to maximize the total reward it receives over time.","sidebar":"defaultSidebar"},"nvidia-isaac/visual-slam":{"id":"nvidia-isaac/visual-slam","title":"Visual SLAM","description":"Visual Simultaneous Localization and Mapping (VSLAM) is a technique that allows a robot to build a map of its environment while simultaneously tracking its own location within that map. VSLAM is a crucial technology for autonomous navigation, as it allows a robot to navigate in an unknown environment without the need for an external localization system such as GPS.","sidebar":"defaultSidebar"},"ros-2/introduction-to-ros2":{"id":"ros-2/introduction-to-ros2","title":"Introduction to ROS 2","description":"The Robot Operating System (ROS) is a flexible framework for writing robot software. It is a collection of tools, libraries, and conventions that aim to simplify the task of creating complex and robust robot behavior across a wide variety of robotic platforms.","sidebar":"defaultSidebar"},"ros-2/rclpy":{"id":"ros-2/rclpy","title":"Bridging Python Agents to ROS controllers using rclpy","description":"rclpy is the Python client library for ROS 2. It provides a Pythonic interface to the ROS 2 ecosystem, allowing you to create nodes, publishers, subscribers, and more.","sidebar":"defaultSidebar"},"ros-2/ros2-architecture":{"id":"ros-2/ros2-architecture","title":"ROS 2 Architecture","description":"The architecture of ROS 2 is designed to be modular and scalable, allowing you to build complex robotic systems from a set of smaller, reusable components. The core of the ROS 2 architecture is the ROS 2 graph, which is a network of nodes that communicate with each other using topics, services, and actions.","sidebar":"defaultSidebar"},"ros-2/ros2-nodes-topics-services":{"id":"ros-2/ros2-nodes-topics-services","title":"ROS 2 Nodes, Topics, and Services","description":"In this section, we will dive into the practical aspects of working with ROS 2 nodes, topics, and services. We will learn how to create and run ROS 2 nodes, and how to use topics and services for communication.","sidebar":"defaultSidebar"},"ros-2/urdf":{"id":"ros-2/urdf","title":"Understanding URDF","description":"The Unified Robot Description Format (URDF) is an XML format for representing a robot model. In ROS, URDF is the standard way to describe the physical structure of a robot, including its links, joints, and sensors.","sidebar":"defaultSidebar"},"vla/capstone-project":{"id":"vla/capstone-project","title":"Capstone Project","description":"The capstone project for this book is The Autonomous Humanoid. In this project, you will bring together everything you have learned to create a simulated humanoid robot that can understand natural language commands, navigate its environment, and manipulate objects.","sidebar":"defaultSidebar"},"vla/conversational-ai":{"id":"vla/conversational-ai","title":"Conversational AI for Robotics","description":"Conversational AI is a field of artificial intelligence that focuses on enabling computers to understand and generate human language. In robotics, conversational AI can be used to create a more natural and intuitive interface for controlling and interacting with robots.","sidebar":"defaultSidebar"},"vla/humanoid-kinematics":{"id":"vla/humanoid-kinematics","title":"Humanoid Kinematics and Manipulation","description":"Kinematics is the study of motion without considering the forces that cause it. In robotics, kinematics is used to describe the motion of a robot's joints and links. For humanoid robots, which have a large number of joints, kinematics is a challenging but essential topic.","sidebar":"defaultSidebar"},"vla/introduction-to-vla":{"id":"vla/introduction-to-vla","title":"Introduction to Vision-Language-Action (VLA) Models","description":"Vision-Language-Action (VLA) models represent a significant leap forward in the quest for truly intelligent and autonomous robots. These models are designed to bridge the gap between perception, language, and action, allowing a robot to understand its environment, reason about it, and take actions to achieve its goals.","sidebar":"defaultSidebar"},"weekly-breakdown/introduction":{"id":"weekly-breakdown/introduction","title":"Weekly Breakdown","description":"Weeks 1-2: Introduction to Physical AI*","sidebar":"defaultSidebar"},"why-physical-ai-matters/introduction":{"id":"why-physical-ai-matters/introduction","title":"Why Physical AI Matters","description":"Humanoid robots are poised to excel in our human-centered world because they share our physical form and can be trained with abundant data from interacting in human environments. This represents a significant transition from AI models confined to digital environments to embodied intelligence that operates in physical space.","sidebar":"defaultSidebar"}}}}
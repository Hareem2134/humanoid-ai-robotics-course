{"allContent":{"docusaurus-plugin-css-cascade-layers":{},"docusaurus-plugin-content-docs":{"default":{"loadedVersions":[{"versionName":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","path":"/humanoid-ai-robotics-course/docs","tagsPath":"/humanoid-ai-robotics-course/docs/tags","editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs","isLast":true,"routePriority":-1,"sidebarFilePath":"C:\\Users\\haree\\Downloads\\humanoid-ai-robotics-course\\frontend\\docusaurus-site\\sidebars.js","contentPath":"C:\\Users\\haree\\Downloads\\humanoid-ai-robotics-course\\frontend\\docusaurus-site\\docs","docs":[{"id":"assessments/introduction","title":"Assessments","description":"*   ROS 2 package development project","source":"@site/docs/08-assessments/01-introduction.md","sourceDirName":"08-assessments","slug":"/assessments/introduction","permalink":"/humanoid-ai-robotics-course/docs/assessments/introduction","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/08-assessments/01-introduction.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{},"sidebar":"defaultSidebar","previous":{"title":"Weekly Breakdown","permalink":"/humanoid-ai-robotics-course/docs/weekly-breakdown/introduction"},"next":{"title":"Hardware Requirements","permalink":"/humanoid-ai-robotics-course/docs/hardware-requirements/introduction"}},{"id":"gazebo-and-unity/gazebo","title":"Gazebo for Simulation","description":"Gazebo is a powerful and widely used open-source robotics simulator. It allows you to simulate robots in complex and realistic environments, with support for a wide range of sensors and physics engines.","source":"@site/docs/02-gazebo-and-unity/02-gazebo.md","sourceDirName":"02-gazebo-and-unity","slug":"/gazebo-and-unity/gazebo","permalink":"/humanoid-ai-robotics-course/docs/gazebo-and-unity/gazebo","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/02-gazebo-and-unity/02-gazebo.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Gazebo for Simulation"},"sidebar":"defaultSidebar","previous":{"title":"Introduction to Simulation","permalink":"/humanoid-ai-robotics-course/docs/gazebo-and-unity/introduction-to-simulation"},"next":{"title":"Unity for Photorealistic Simulation","permalink":"/humanoid-ai-robotics-course/docs/gazebo-and-unity/unity"}},{"id":"gazebo-and-unity/introduction-to-simulation","title":"Introduction to Simulation","description":"Simulation is an indispensable tool in the field of robotics. It allows engineers, developers, and researchers to create a virtual representation of a robot and its environment, enabling them to test and validate their designs, algorithms, and software before deploying them on physical hardware.","source":"@site/docs/02-gazebo-and-unity/01-introduction-to-simulation.md","sourceDirName":"02-gazebo-and-unity","slug":"/gazebo-and-unity/introduction-to-simulation","permalink":"/humanoid-ai-robotics-course/docs/gazebo-and-unity/introduction-to-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/02-gazebo-and-unity/01-introduction-to-simulation.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Introduction to Simulation"},"sidebar":"defaultSidebar","previous":{"title":"Understanding URDF","permalink":"/humanoid-ai-robotics-course/docs/ros-2/urdf"},"next":{"title":"Gazebo for Simulation","permalink":"/humanoid-ai-robotics-course/docs/gazebo-and-unity/gazebo"}},{"id":"gazebo-and-unity/simulating-sensors","title":"Simulating Sensors","description":"Simulating sensors is a critical aspect of robotics simulation. It allows you to test your robot's perception and control algorithms in a virtual environment before deploying them on a physical robot. In this section, we will explore how to simulate some of the most common sensors in robotics.","source":"@site/docs/02-gazebo-and-unity/04-simulating-sensors.md","sourceDirName":"02-gazebo-and-unity","slug":"/gazebo-and-unity/simulating-sensors","permalink":"/humanoid-ai-robotics-course/docs/gazebo-and-unity/simulating-sensors","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/02-gazebo-and-unity/04-simulating-sensors.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Simulating Sensors"},"sidebar":"defaultSidebar","previous":{"title":"Unity for Photorealistic Simulation","permalink":"/humanoid-ai-robotics-course/docs/gazebo-and-unity/unity"},"next":{"title":"Introduction to NVIDIA Isaac Sim","permalink":"/humanoid-ai-robotics-course/docs/nvidia-isaac/introduction-to-isaac-sim"}},{"id":"gazebo-and-unity/unity","title":"Unity for Photorealistic Simulation","description":"Unity is a powerful and popular game engine that is increasingly being used for robotics simulation. Its advanced rendering capabilities, realistic physics, and intuitive user interface make it an excellent choice for creating photorealistic and interactive simulation environments.","source":"@site/docs/02-gazebo-and-unity/03-unity.md","sourceDirName":"02-gazebo-and-unity","slug":"/gazebo-and-unity/unity","permalink":"/humanoid-ai-robotics-course/docs/gazebo-and-unity/unity","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/02-gazebo-and-unity/03-unity.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Unity for Photorealistic Simulation"},"sidebar":"defaultSidebar","previous":{"title":"Gazebo for Simulation","permalink":"/humanoid-ai-robotics-course/docs/gazebo-and-unity/gazebo"},"next":{"title":"Simulating Sensors","permalink":"/humanoid-ai-robotics-course/docs/gazebo-and-unity/simulating-sensors"}},{"id":"glossary/glossary","title":"Glossary","description":"This glossary provides definitions for key terms and acronyms used throughout this book.","source":"@site/docs/05-glossary/01-glossary.md","sourceDirName":"05-glossary","slug":"/glossary/glossary","permalink":"/humanoid-ai-robotics-course/docs/glossary/glossary","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/05-glossary/01-glossary.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Glossary"},"sidebar":"defaultSidebar","previous":{"title":"Capstone Project","permalink":"/humanoid-ai-robotics-course/docs/vla/capstone-project"},"next":{"title":"Why Physical AI Matters","permalink":"/humanoid-ai-robotics-course/docs/why-physical-ai-matters/introduction"}},{"id":"hardware-requirements/introduction","title":"Hardware Requirements","description":"This course is technically demanding. It sits at the intersection of three heavy computational loads: Physics Simulation (Isaac Sim/Gazebo), Visual Perception (SLAM/Computer Vision), and Generative AI (LLMs/VLA).","source":"@site/docs/09-hardware-requirements/01-introduction.md","sourceDirName":"09-hardware-requirements","slug":"/hardware-requirements/introduction","permalink":"/humanoid-ai-robotics-course/docs/hardware-requirements/introduction","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/09-hardware-requirements/01-introduction.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{},"sidebar":"defaultSidebar","previous":{"title":"Assessments","permalink":"/humanoid-ai-robotics-course/docs/assessments/introduction"},"next":{"title":"Introduction","permalink":"/humanoid-ai-robotics-course/docs/"}},{"id":"intro","title":"Introduction","description":"Welcome to \"Physical AI & Humanoid Robotics: A Comprehensive Guide\". This book is designed to provide industry practitioners with a comprehensive understanding of humanoid robotics and physical AI.","source":"@site/docs/intro.md","sourceDirName":".","slug":"/","permalink":"/humanoid-ai-robotics-course/docs/","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/intro.md","tags":[],"version":"current","frontMatter":{"id":"intro","title":"Introduction","slug":"/"},"sidebar":"defaultSidebar","previous":{"title":"Hardware Requirements","permalink":"/humanoid-ai-robotics-course/docs/hardware-requirements/introduction"}},{"id":"introduction/learning-outcomes","title":"Learning Outcomes","description":"Upon completion of this book, you will be able to:","source":"@site/docs/00-introduction/02-learning-outcomes.md","sourceDirName":"00-introduction","slug":"/introduction/learning-outcomes","permalink":"/humanoid-ai-robotics-course/docs/introduction/learning-outcomes","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/00-introduction/02-learning-outcomes.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Learning Outcomes"},"sidebar":"defaultSidebar","previous":{"title":"Overview","permalink":"/humanoid-ai-robotics-course/docs/introduction/overview"},"next":{"title":"Hardware and Software Requirements","permalink":"/humanoid-ai-robotics-course/docs/introduction/requirements"}},{"id":"introduction/overview","title":"Overview","description":"Welcome to \"Physical AI & Humanoid Robotics: A Comprehensive Guide\". This book is designed to provide industry practitioners with a comprehensive understanding of humanoid robotics and physical AI.","source":"@site/docs/00-introduction/01-overview.md","sourceDirName":"00-introduction","slug":"/introduction/overview","permalink":"/humanoid-ai-robotics-course/docs/introduction/overview","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/00-introduction/01-overview.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Overview"},"sidebar":"defaultSidebar","next":{"title":"Learning Outcomes","permalink":"/humanoid-ai-robotics-course/docs/introduction/learning-outcomes"}},{"id":"introduction/requirements","title":"Hardware and Software Requirements","description":"To follow along with the examples in this book, you will need the following hardware and software:","source":"@site/docs/00-introduction/03-requirements.md","sourceDirName":"00-introduction","slug":"/introduction/requirements","permalink":"/humanoid-ai-robotics-course/docs/introduction/requirements","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/00-introduction/03-requirements.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Hardware and Software Requirements"},"sidebar":"defaultSidebar","previous":{"title":"Learning Outcomes","permalink":"/humanoid-ai-robotics-course/docs/introduction/learning-outcomes"},"next":{"title":"Introduction to ROS 2","permalink":"/humanoid-ai-robotics-course/docs/ros-2/introduction-to-ros2"}},{"id":"learning-outcomes/introduction","title":"Learning Outcomes","description":"*   Understand Physical AI principles and embodied intelligence","source":"@site/docs/06-learning-outcomes/01-introduction.md","sourceDirName":"06-learning-outcomes","slug":"/learning-outcomes/introduction","permalink":"/humanoid-ai-robotics-course/docs/learning-outcomes/introduction","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/06-learning-outcomes/01-introduction.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{},"sidebar":"defaultSidebar","previous":{"title":"Why Physical AI Matters","permalink":"/humanoid-ai-robotics-course/docs/why-physical-ai-matters/introduction"},"next":{"title":"Weekly Breakdown","permalink":"/humanoid-ai-robotics-course/docs/weekly-breakdown/introduction"}},{"id":"nvidia-isaac/introduction-to-isaac-sim","title":"Introduction to NVIDIA Isaac Sim","description":"NVIDIA Isaac Sim is a powerful robotics simulation and synthetic data generation platform. It is built on top of the NVIDIA Omniverse platform, which is a real-time 3D collaboration and simulation platform. Isaac Sim leverages the power of NVIDIA's RTX GPUs to provide a realistic and physically accurate simulation environment for developing, testing, and training AI-based robots.","source":"@site/docs/03-nvidia-isaac/01-introduction-to-isaac-sim.md","sourceDirName":"03-nvidia-isaac","slug":"/nvidia-isaac/introduction-to-isaac-sim","permalink":"/humanoid-ai-robotics-course/docs/nvidia-isaac/introduction-to-isaac-sim","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/03-nvidia-isaac/01-introduction-to-isaac-sim.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Introduction to NVIDIA Isaac Sim"},"sidebar":"defaultSidebar","previous":{"title":"Simulating Sensors","permalink":"/humanoid-ai-robotics-course/docs/gazebo-and-unity/simulating-sensors"},"next":{"title":"Visual SLAM","permalink":"/humanoid-ai-robotics-course/docs/nvidia-isaac/visual-slam"}},{"id":"nvidia-isaac/nav2","title":"Nav2 for Path Planning","description":"Nav2 is the second generation of the ROS Navigation Stack. It is a powerful and flexible navigation framework that can be used to control a robot as it moves from one location to another. Nav2 is highly configurable and can be used with a wide variety of robots, including differential drive robots, omnidirectional robots, and even bipedal humanoids.","source":"@site/docs/03-nvidia-isaac/03-nav2.md","sourceDirName":"03-nvidia-isaac","slug":"/nvidia-isaac/nav2","permalink":"/humanoid-ai-robotics-course/docs/nvidia-isaac/nav2","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/03-nvidia-isaac/03-nav2.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Nav2 for Path Planning"},"sidebar":"defaultSidebar","previous":{"title":"Visual SLAM","permalink":"/humanoid-ai-robotics-course/docs/nvidia-isaac/visual-slam"},"next":{"title":"Reinforcement Learning for Robotics","permalink":"/humanoid-ai-robotics-course/docs/nvidia-isaac/reinforcement-learning"}},{"id":"nvidia-isaac/reinforcement-learning","title":"Reinforcement Learning for Robotics","description":"Reinforcement Learning (RL) is a powerful machine learning paradigm that is well-suited for robotics. In RL, an agent learns to achieve a goal by interacting with an environment. The agent receives a reward for each action it takes, and its goal is to maximize the total reward it receives over time.","source":"@site/docs/03-nvidia-isaac/04-reinforcement-learning.md","sourceDirName":"03-nvidia-isaac","slug":"/nvidia-isaac/reinforcement-learning","permalink":"/humanoid-ai-robotics-course/docs/nvidia-isaac/reinforcement-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/03-nvidia-isaac/04-reinforcement-learning.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Reinforcement Learning for Robotics"},"sidebar":"defaultSidebar","previous":{"title":"Nav2 for Path Planning","permalink":"/humanoid-ai-robotics-course/docs/nvidia-isaac/nav2"},"next":{"title":"Introduction to Vision-Language-Action (VLA) Models","permalink":"/humanoid-ai-robotics-course/docs/vla/introduction-to-vla"}},{"id":"nvidia-isaac/visual-slam","title":"Visual SLAM","description":"Visual Simultaneous Localization and Mapping (VSLAM) is a technique that allows a robot to build a map of its environment while simultaneously tracking its own location within that map. VSLAM is a crucial technology for autonomous navigation, as it allows a robot to navigate in an unknown environment without the need for an external localization system such as GPS.","source":"@site/docs/03-nvidia-isaac/02-visual-slam.md","sourceDirName":"03-nvidia-isaac","slug":"/nvidia-isaac/visual-slam","permalink":"/humanoid-ai-robotics-course/docs/nvidia-isaac/visual-slam","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/03-nvidia-isaac/02-visual-slam.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Visual SLAM"},"sidebar":"defaultSidebar","previous":{"title":"Introduction to NVIDIA Isaac Sim","permalink":"/humanoid-ai-robotics-course/docs/nvidia-isaac/introduction-to-isaac-sim"},"next":{"title":"Nav2 for Path Planning","permalink":"/humanoid-ai-robotics-course/docs/nvidia-isaac/nav2"}},{"id":"ros-2/introduction-to-ros2","title":"Introduction to ROS 2","description":"The Robot Operating System (ROS) is a flexible framework for writing robot software. It is a collection of tools, libraries, and conventions that aim to simplify the task of creating complex and robust robot behavior across a wide variety of robotic platforms.","source":"@site/docs/01-ros-2/01-introduction-to-ros2.md","sourceDirName":"01-ros-2","slug":"/ros-2/introduction-to-ros2","permalink":"/humanoid-ai-robotics-course/docs/ros-2/introduction-to-ros2","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/01-ros-2/01-introduction-to-ros2.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Introduction to ROS 2"},"sidebar":"defaultSidebar","previous":{"title":"Hardware and Software Requirements","permalink":"/humanoid-ai-robotics-course/docs/introduction/requirements"},"next":{"title":"ROS 2 Architecture","permalink":"/humanoid-ai-robotics-course/docs/ros-2/ros2-architecture"}},{"id":"ros-2/rclpy","title":"Bridging Python Agents to ROS controllers using rclpy","description":"rclpy is the Python client library for ROS 2. It provides a Pythonic interface to the ROS 2 ecosystem, allowing you to create nodes, publishers, subscribers, and more.","source":"@site/docs/01-ros-2/04-rclpy.md","sourceDirName":"01-ros-2","slug":"/ros-2/rclpy","permalink":"/humanoid-ai-robotics-course/docs/ros-2/rclpy","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/01-ros-2/04-rclpy.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Bridging Python Agents to ROS controllers using rclpy"},"sidebar":"defaultSidebar","previous":{"title":"ROS 2 Nodes, Topics, and Services","permalink":"/humanoid-ai-robotics-course/docs/ros-2/ros2-nodes-topics-services"},"next":{"title":"Understanding URDF","permalink":"/humanoid-ai-robotics-course/docs/ros-2/urdf"}},{"id":"ros-2/ros2-architecture","title":"ROS 2 Architecture","description":"The architecture of ROS 2 is designed to be modular and scalable, allowing you to build complex robotic systems from a set of smaller, reusable components. The core of the ROS 2 architecture is the ROS 2 graph, which is a network of nodes that communicate with each other using topics, services, and actions.","source":"@site/docs/01-ros-2/02-ros2-architecture.md","sourceDirName":"01-ros-2","slug":"/ros-2/ros2-architecture","permalink":"/humanoid-ai-robotics-course/docs/ros-2/ros2-architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/01-ros-2/02-ros2-architecture.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"ROS 2 Architecture"},"sidebar":"defaultSidebar","previous":{"title":"Introduction to ROS 2","permalink":"/humanoid-ai-robotics-course/docs/ros-2/introduction-to-ros2"},"next":{"title":"ROS 2 Nodes, Topics, and Services","permalink":"/humanoid-ai-robotics-course/docs/ros-2/ros2-nodes-topics-services"}},{"id":"ros-2/ros2-nodes-topics-services","title":"ROS 2 Nodes, Topics, and Services","description":"In this section, we will dive into the practical aspects of working with ROS 2 nodes, topics, and services. We will learn how to create and run ROS 2 nodes, and how to use topics and services for communication.","source":"@site/docs/01-ros-2/03-ros2-nodes-topics-services.md","sourceDirName":"01-ros-2","slug":"/ros-2/ros2-nodes-topics-services","permalink":"/humanoid-ai-robotics-course/docs/ros-2/ros2-nodes-topics-services","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/01-ros-2/03-ros2-nodes-topics-services.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"ROS 2 Nodes, Topics, and Services"},"sidebar":"defaultSidebar","previous":{"title":"ROS 2 Architecture","permalink":"/humanoid-ai-robotics-course/docs/ros-2/ros2-architecture"},"next":{"title":"Bridging Python Agents to ROS controllers using rclpy","permalink":"/humanoid-ai-robotics-course/docs/ros-2/rclpy"}},{"id":"ros-2/urdf","title":"Understanding URDF","description":"The Unified Robot Description Format (URDF) is an XML format for representing a robot model. In ROS, URDF is the standard way to describe the physical structure of a robot, including its links, joints, and sensors.","source":"@site/docs/01-ros-2/05-urdf.md","sourceDirName":"01-ros-2","slug":"/ros-2/urdf","permalink":"/humanoid-ai-robotics-course/docs/ros-2/urdf","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/01-ros-2/05-urdf.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Understanding URDF"},"sidebar":"defaultSidebar","previous":{"title":"Bridging Python Agents to ROS controllers using rclpy","permalink":"/humanoid-ai-robotics-course/docs/ros-2/rclpy"},"next":{"title":"Introduction to Simulation","permalink":"/humanoid-ai-robotics-course/docs/gazebo-and-unity/introduction-to-simulation"}},{"id":"vla/capstone-project","title":"Capstone Project","description":"The capstone project for this book is The Autonomous Humanoid. In this project, you will bring together everything you have learned to create a simulated humanoid robot that can understand natural language commands, navigate its environment, and manipulate objects.","source":"@site/docs/04-vla/04-capstone-project.md","sourceDirName":"04-vla","slug":"/vla/capstone-project","permalink":"/humanoid-ai-robotics-course/docs/vla/capstone-project","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/04-vla/04-capstone-project.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Capstone Project"},"sidebar":"defaultSidebar","previous":{"title":"Conversational AI for Robotics","permalink":"/humanoid-ai-robotics-course/docs/vla/conversational-ai"},"next":{"title":"Glossary","permalink":"/humanoid-ai-robotics-course/docs/glossary/glossary"}},{"id":"vla/conversational-ai","title":"Conversational AI for Robotics","description":"Conversational AI is a field of artificial intelligence that focuses on enabling computers to understand and generate human language. In robotics, conversational AI can be used to create a more natural and intuitive interface for controlling and interacting with robots.","source":"@site/docs/04-vla/03-conversational-ai.md","sourceDirName":"04-vla","slug":"/vla/conversational-ai","permalink":"/humanoid-ai-robotics-course/docs/vla/conversational-ai","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/04-vla/03-conversational-ai.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Conversational AI for Robotics"},"sidebar":"defaultSidebar","previous":{"title":"Humanoid Kinematics and Manipulation","permalink":"/humanoid-ai-robotics-course/docs/vla/humanoid-kinematics"},"next":{"title":"Capstone Project","permalink":"/humanoid-ai-robotics-course/docs/vla/capstone-project"}},{"id":"vla/humanoid-kinematics","title":"Humanoid Kinematics and Manipulation","description":"Kinematics is the study of motion without considering the forces that cause it. In robotics, kinematics is used to describe the motion of a robot's joints and links. For humanoid robots, which have a large number of joints, kinematics is a challenging but essential topic.","source":"@site/docs/04-vla/02-humanoid-kinematics.md","sourceDirName":"04-vla","slug":"/vla/humanoid-kinematics","permalink":"/humanoid-ai-robotics-course/docs/vla/humanoid-kinematics","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/04-vla/02-humanoid-kinematics.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Humanoid Kinematics and Manipulation"},"sidebar":"defaultSidebar","previous":{"title":"Introduction to Vision-Language-Action (VLA) Models","permalink":"/humanoid-ai-robotics-course/docs/vla/introduction-to-vla"},"next":{"title":"Conversational AI for Robotics","permalink":"/humanoid-ai-robotics-course/docs/vla/conversational-ai"}},{"id":"vla/introduction-to-vla","title":"Introduction to Vision-Language-Action (VLA) Models","description":"Vision-Language-Action (VLA) models represent a significant leap forward in the quest for truly intelligent and autonomous robots. These models are designed to bridge the gap between perception, language, and action, allowing a robot to understand its environment, reason about it, and take actions to achieve its goals.","source":"@site/docs/04-vla/01-introduction-to-vla.md","sourceDirName":"04-vla","slug":"/vla/introduction-to-vla","permalink":"/humanoid-ai-robotics-course/docs/vla/introduction-to-vla","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/04-vla/01-introduction-to-vla.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Introduction to Vision-Language-Action (VLA) Models"},"sidebar":"defaultSidebar","previous":{"title":"Reinforcement Learning for Robotics","permalink":"/humanoid-ai-robotics-course/docs/nvidia-isaac/reinforcement-learning"},"next":{"title":"Humanoid Kinematics and Manipulation","permalink":"/humanoid-ai-robotics-course/docs/vla/humanoid-kinematics"}},{"id":"weekly-breakdown/introduction","title":"Weekly Breakdown","description":"Weeks 1-2: Introduction to Physical AI*","source":"@site/docs/07-weekly-breakdown/01-introduction.md","sourceDirName":"07-weekly-breakdown","slug":"/weekly-breakdown/introduction","permalink":"/humanoid-ai-robotics-course/docs/weekly-breakdown/introduction","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/07-weekly-breakdown/01-introduction.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{},"sidebar":"defaultSidebar","previous":{"title":"Learning Outcomes","permalink":"/humanoid-ai-robotics-course/docs/learning-outcomes/introduction"},"next":{"title":"Assessments","permalink":"/humanoid-ai-robotics-course/docs/assessments/introduction"}},{"id":"why-physical-ai-matters/introduction","title":"Why Physical AI Matters","description":"Humanoid robots are poised to excel in our human-centered world because they share our physical form and can be trained with abundant data from interacting in human environments. This represents a significant transition from AI models confined to digital environments to embodied intelligence that operates in physical space.","source":"@site/docs/05-why-physical-ai-matters/01-introduction.md","sourceDirName":"05-why-physical-ai-matters","slug":"/why-physical-ai-matters/introduction","permalink":"/humanoid-ai-robotics-course/docs/why-physical-ai-matters/introduction","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/05-why-physical-ai-matters/01-introduction.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{},"sidebar":"defaultSidebar","previous":{"title":"Glossary","permalink":"/humanoid-ai-robotics-course/docs/glossary/glossary"},"next":{"title":"Learning Outcomes","permalink":"/humanoid-ai-robotics-course/docs/learning-outcomes/introduction"}}],"drafts":[],"sidebars":{"defaultSidebar":[{"type":"category","label":"Introduction","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"introduction/overview"},{"type":"doc","id":"introduction/learning-outcomes"},{"type":"doc","id":"introduction/requirements"}]},{"type":"category","label":"Module 1: ROS 2","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"ros-2/introduction-to-ros2"},{"type":"doc","id":"ros-2/ros2-architecture"},{"type":"doc","id":"ros-2/ros2-nodes-topics-services"},{"type":"doc","id":"ros-2/rclpy"},{"type":"doc","id":"ros-2/urdf"}]},{"type":"category","label":"Module 2: Digital Twins","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"gazebo-and-unity/introduction-to-simulation"},{"type":"doc","id":"gazebo-and-unity/gazebo"},{"type":"doc","id":"gazebo-and-unity/unity"},{"type":"doc","id":"gazebo-and-unity/simulating-sensors"}]},{"type":"category","label":"Module 3: NVIDIA Isaac","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"nvidia-isaac/introduction-to-isaac-sim"},{"type":"doc","id":"nvidia-isaac/visual-slam"},{"type":"doc","id":"nvidia-isaac/nav2"},{"type":"doc","id":"nvidia-isaac/reinforcement-learning"}]},{"type":"category","label":"Module 4: VLA & Humanoid Robotics","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"vla/introduction-to-vla"},{"type":"doc","id":"vla/humanoid-kinematics"},{"type":"doc","id":"vla/conversational-ai"},{"type":"doc","id":"vla/capstone-project"}]},{"type":"category","label":"Glossary","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"glossary/glossary"}]},{"type":"category","label":"Why Physical AI Matters","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"why-physical-ai-matters/introduction"}]},{"type":"category","label":"Learning Outcomes","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"learning-outcomes/introduction"}]},{"type":"category","label":"Weekly Breakdown","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"weekly-breakdown/introduction"}]},{"type":"category","label":"Assessments","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"assessments/introduction"}]},{"type":"category","label":"Hardware Requirements","collapsible":true,"collapsed":true,"items":[{"type":"doc","id":"hardware-requirements/introduction"}]},{"type":"doc","id":"intro"}]}}]}},"docusaurus-plugin-content-blog":{"default":{"blogSidebarTitle":"Recent posts","blogPosts":[],"blogListPaginated":[],"blogTags":{},"blogTagsListPath":"/humanoid-ai-robotics-course/blog/tags"}},"docusaurus-plugin-content-pages":{"default":[{"type":"jsx","permalink":"/humanoid-ai-robotics-course/","source":"@site/src/pages/index.js"}]},"docusaurus-plugin-debug":{},"docusaurus-plugin-svgr":{},"docusaurus-theme-classic":{},"docusaurus-chatbot-plugin":{},"docusaurus-bootstrap-plugin":{},"docusaurus-mdx-fallback-plugin":{}}}
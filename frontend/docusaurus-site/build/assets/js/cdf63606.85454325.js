"use strict";(globalThis.webpackChunkdocusaurus_site=globalThis.webpackChunkdocusaurus_site||[]).push([[8439],{4993:i=>{i.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"defaultSidebar":[{"type":"category","label":"Introduction","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-ai-robotics-course/docs/introduction/overview","label":"Overview","docId":"introduction/overview","unlisted":false},{"type":"link","href":"/humanoid-ai-robotics-course/docs/introduction/learning-outcomes","label":"Learning Outcomes","docId":"introduction/learning-outcomes","unlisted":false},{"type":"link","href":"/humanoid-ai-robotics-course/docs/introduction/requirements","label":"Hardware and Software Requirements","docId":"introduction/requirements","unlisted":false}]},{"type":"category","label":"Module 1: ROS 2","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-ai-robotics-course/docs/ros-2/introduction-to-ros2","label":"Introduction to ROS 2","docId":"ros-2/introduction-to-ros2","unlisted":false},{"type":"link","href":"/humanoid-ai-robotics-course/docs/ros-2/ros2-architecture","label":"ROS 2 Architecture","docId":"ros-2/ros2-architecture","unlisted":false},{"type":"link","href":"/humanoid-ai-robotics-course/docs/ros-2/ros2-nodes-topics-services","label":"ROS 2 Nodes, Topics, and Services","docId":"ros-2/ros2-nodes-topics-services","unlisted":false},{"type":"link","href":"/humanoid-ai-robotics-course/docs/ros-2/rclpy","label":"Bridging Python Agents to ROS controllers using rclpy","docId":"ros-2/rclpy","unlisted":false},{"type":"link","href":"/humanoid-ai-robotics-course/docs/ros-2/urdf","label":"Understanding URDF","docId":"ros-2/urdf","unlisted":false}]},{"type":"category","label":"Module 2: Digital Twins","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-ai-robotics-course/docs/gazebo-and-unity/introduction-to-simulation","label":"Introduction to Simulation","docId":"gazebo-and-unity/introduction-to-simulation","unlisted":false},{"type":"link","href":"/humanoid-ai-robotics-course/docs/gazebo-and-unity/gazebo","label":"Gazebo for Simulation","docId":"gazebo-and-unity/gazebo","unlisted":false},{"type":"link","href":"/humanoid-ai-robotics-course/docs/gazebo-and-unity/unity","label":"Unity for Photorealistic Simulation","docId":"gazebo-and-unity/unity","unlisted":false},{"type":"link","href":"/humanoid-ai-robotics-course/docs/gazebo-and-unity/simulating-sensors","label":"Simulating Sensors","docId":"gazebo-and-unity/simulating-sensors","unlisted":false}]},{"type":"category","label":"Module 3: NVIDIA Isaac","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-ai-robotics-course/docs/nvidia-isaac/introduction-to-isaac-sim","label":"Introduction to NVIDIA Isaac Sim","docId":"nvidia-isaac/introduction-to-isaac-sim","unlisted":false},{"type":"link","href":"/humanoid-ai-robotics-course/docs/nvidia-isaac/visual-slam","label":"Visual SLAM","docId":"nvidia-isaac/visual-slam","unlisted":false},{"type":"link","href":"/humanoid-ai-robotics-course/docs/nvidia-isaac/nav2","label":"Nav2 for Path Planning","docId":"nvidia-isaac/nav2","unlisted":false},{"type":"link","href":"/humanoid-ai-robotics-course/docs/nvidia-isaac/reinforcement-learning","label":"Reinforcement Learning for Robotics","docId":"nvidia-isaac/reinforcement-learning","unlisted":false}]},{"type":"category","label":"Module 4: VLA & Humanoid Robotics","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-ai-robotics-course/docs/vla/introduction-to-vla","label":"Introduction to Vision-Language-Action (VLA) Models","docId":"vla/introduction-to-vla","unlisted":false},{"type":"link","href":"/humanoid-ai-robotics-course/docs/vla/humanoid-kinematics","label":"Humanoid Kinematics and Manipulation","docId":"vla/humanoid-kinematics","unlisted":false},{"type":"link","href":"/humanoid-ai-robotics-course/docs/vla/conversational-ai","label":"Conversational AI for Robotics","docId":"vla/conversational-ai","unlisted":false},{"type":"link","href":"/humanoid-ai-robotics-course/docs/vla/capstone-project","label":"Capstone Project","docId":"vla/capstone-project","unlisted":false}]},{"type":"category","label":"Glossary","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-ai-robotics-course/docs/glossary/glossary","label":"Glossary","docId":"glossary/glossary","unlisted":false}]},{"type":"category","label":"Why Physical AI Matters","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-ai-robotics-course/docs/why-physical-ai-matters/introduction","label":"Why Physical AI Matters","docId":"why-physical-ai-matters/introduction","unlisted":false}]},{"type":"category","label":"Learning Outcomes","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-ai-robotics-course/docs/learning-outcomes/introduction","label":"Learning Outcomes","docId":"learning-outcomes/introduction","unlisted":false}]},{"type":"category","label":"Weekly Breakdown","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-ai-robotics-course/docs/weekly-breakdown/introduction","label":"Weekly Breakdown","docId":"weekly-breakdown/introduction","unlisted":false}]},{"type":"category","label":"Assessments","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-ai-robotics-course/docs/assessments/introduction","label":"Assessments","docId":"assessments/introduction","unlisted":false}]},{"type":"category","label":"Hardware Requirements","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/humanoid-ai-robotics-course/docs/hardware-requirements/introduction","label":"Hardware Requirements","docId":"hardware-requirements/introduction","unlisted":false}]}]},"docs":{"assessments/introduction":{"id":"assessments/introduction","title":"Assessments","description":"*   ROS 2 package development project","sidebar":"defaultSidebar"},"gazebo-and-unity/gazebo":{"id":"gazebo-and-unity/gazebo","title":"Gazebo for Simulation","description":"Focus: Physics simulation and environment building.","sidebar":"defaultSidebar"},"gazebo-and-unity/introduction-to-simulation":{"id":"gazebo-and-unity/introduction-to-simulation","title":"Introduction to Simulation","description":"Simulation is a critical tool in robotics, allowing us to test and develop robots in a virtual environment before deploying them in the real world. This reduces the risk of damage to expensive hardware and allows for rapid iteration on designs and algorithms.","sidebar":"defaultSidebar"},"gazebo-and-unity/simulating-sensors":{"id":"gazebo-and-unity/simulating-sensors","title":"Simulating Sensors","description":"This section will cover how to simulate sensors such as LiDAR, Depth Cameras, and IMUs in Gazebo and Unity.","sidebar":"defaultSidebar"},"gazebo-and-unity/unity":{"id":"gazebo-and-unity/unity","title":"Unity for Photorealistic Simulation","description":"This section will cover how to use Unity for high-fidelity rendering and human-robot interaction.","sidebar":"defaultSidebar"},"glossary/glossary":{"id":"glossary/glossary","title":"Glossary","description":"This section contains definitions of key terms used throughout this book.","sidebar":"defaultSidebar"},"hardware-requirements/introduction":{"id":"hardware-requirements/introduction","title":"Hardware Requirements","description":"This course is technically demanding. It sits at the intersection of three heavy computational loads: Physics Simulation (Isaac Sim/Gazebo), Visual Perception (SLAM/Computer Vision), and Generative AI (LLMs/VLA).","sidebar":"defaultSidebar"},"introduction/learning-outcomes":{"id":"introduction/learning-outcomes","title":"Learning Outcomes","description":"Upon completion of this book, you will be able to:","sidebar":"defaultSidebar"},"introduction/overview":{"id":"introduction/overview","title":"Overview","description":"Welcome to \\"Physical AI & Humanoid Robotics: A Comprehensive Guide\\". This book is designed to provide industry practitioners with a comprehensive understanding of humanoid robotics and physical AI.","sidebar":"defaultSidebar"},"introduction/requirements":{"id":"introduction/requirements","title":"Hardware and Software Requirements","description":"To follow along with the examples in this book, you will need the following hardware and software:","sidebar":"defaultSidebar"},"learning-outcomes/introduction":{"id":"learning-outcomes/introduction","title":"Learning Outcomes","description":"*   Understand Physical AI principles and embodied intelligence","sidebar":"defaultSidebar"},"nvidia-isaac/introduction-to-isaac-sim":{"id":"nvidia-isaac/introduction-to-isaac-sim","title":"Introduction to NVIDIA Isaac Sim","description":"NVIDIA Isaac Sim is a robotics simulation and synthetic data generation platform. It is built on NVIDIA Omniverse and provides a realistic and physically accurate simulation environment for developing, testing, and training AI-based robots.","sidebar":"defaultSidebar"},"nvidia-isaac/nav2":{"id":"nvidia-isaac/nav2","title":"Nav2 for Path Planning","description":"This section will cover how to use the Nav2 stack for path planning with bipedal humanoid robots.","sidebar":"defaultSidebar"},"nvidia-isaac/reinforcement-learning":{"id":"nvidia-isaac/reinforcement-learning","title":"Reinforcement Learning for Robotics","description":"This section will cover the basics of reinforcement learning and how it can be used to train robots to perform complex tasks.","sidebar":"defaultSidebar"},"nvidia-isaac/visual-slam":{"id":"nvidia-isaac/visual-slam","title":"Visual SLAM","description":"Focus: Advanced perception and training.","sidebar":"defaultSidebar"},"ros-2/introduction-to-ros2":{"id":"ros-2/introduction-to-ros2","title":"Introduction to ROS 2","description":"ROS 2 is a set of software libraries and tools that help you build robot applications. It is a complete rewrite of ROS 1, designed to be more reliable, secure, and scalable.","sidebar":"defaultSidebar"},"ros-2/rclpy":{"id":"ros-2/rclpy","title":"Bridging Python Agents to ROS controllers using rclpy","description":"This section will cover how to use the rclpy library to interface with ROS 2 from Python.","sidebar":"defaultSidebar"},"ros-2/ros2-architecture":{"id":"ros-2/ros2-architecture","title":"ROS 2 Architecture","description":"Focus: Middleware for robot control.","sidebar":"defaultSidebar"},"ros-2/ros2-nodes-topics-services":{"id":"ros-2/ros2-nodes-topics-services","title":"ROS 2 Nodes, Topics, and Services","description":"This section will cover the core communication concepts in ROS 2.","sidebar":"defaultSidebar"},"ros-2/urdf":{"id":"ros-2/urdf","title":"Understanding URDF","description":"This section will cover the Unified Robot Description Format (URDF) for describing humanoid robots.","sidebar":"defaultSidebar"},"vla/capstone-project":{"id":"vla/capstone-project","title":"Capstone Project","description":"This section will outline the capstone project for this book: The Autonomous Humanoid.","sidebar":"defaultSidebar"},"vla/conversational-ai":{"id":"vla/conversational-ai","title":"Conversational AI for Robotics","description":"This section will cover how to use tools like OpenAI Whisper to create conversational interfaces for robots.","sidebar":"defaultSidebar"},"vla/humanoid-kinematics":{"id":"vla/humanoid-kinematics","title":"Humanoid Kinematics and Manipulation","description":"Focus: The convergence of LLMs and Robotics.","sidebar":"defaultSidebar"},"vla/introduction-to-vla":{"id":"vla/introduction-to-vla","title":"Introduction to Vision-Language-Action (VLA) Models","description":"Vision-Language-Action (VLA) models are a new class of AI models that can understand and act upon multimodal inputs, including images, text, and speech. These models are at the forefront of AI research and have the potential to revolutionize robotics.","sidebar":"defaultSidebar"},"weekly-breakdown/introduction":{"id":"weekly-breakdown/introduction","title":"Weekly Breakdown","description":"Weeks 1-2: Introduction to Physical AI*","sidebar":"defaultSidebar"},"why-physical-ai-matters/introduction":{"id":"why-physical-ai-matters/introduction","title":"Why Physical AI Matters","description":"Humanoid robots are poised to excel in our human-centered world because they share our physical form and can be trained with abundant data from interacting in human environments. This represents a significant transition from AI models confined to digital environments to embodied intelligence that operates in physical space.","sidebar":"defaultSidebar"}}}}')}}]);
"use strict";(globalThis.webpackChunkdocusaurus_site=globalThis.webpackChunkdocusaurus_site||[]).push([[8918],{3083:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>r,contentTitle:()=>c,default:()=>d,frontMatter:()=>s,metadata:()=>o,toc:()=>u});const o=JSON.parse('{"id":"vla/humanoid-kinematics","title":"Humanoid Kinematics and Manipulation","description":"Focus: The convergence of LLMs and Robotics.","source":"@site/docs/04-vla/02-humanoid-kinematics.md","sourceDirName":"04-vla","slug":"/vla/humanoid-kinematics","permalink":"/humanoid-ai-robotics-course/docs/vla/humanoid-kinematics","draft":false,"unlisted":false,"editUrl":"https://github.com/haree/humanoid-ai-robotics-course/tree/main/docs/04-vla/02-humanoid-kinematics.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Humanoid Kinematics and Manipulation"},"sidebar":"defaultSidebar","previous":{"title":"Introduction to Vision-Language-Action (VLA) Models","permalink":"/humanoid-ai-robotics-course/docs/vla/introduction-to-vla"},"next":{"title":"Conversational AI for Robotics","permalink":"/humanoid-ai-robotics-course/docs/vla/conversational-ai"}}');var t=i(4848),a=i(8453);const s={title:"Humanoid Kinematics and Manipulation"},c="Module 4: Vision-Language-Action (VLA)",r={},u=[];function l(n){const e={h1:"h1",header:"header",li:"li",p:"p",ul:"ul",...(0,a.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,t.jsx)(e.p,{children:"Focus: The convergence of LLMs and Robotics."}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Voice-to-Action: Using OpenAI Whisper for voice commands."}),"\n",(0,t.jsx)(e.li,{children:'Cognitive Planning: Using LLMs to translate natural language ("Clean the room") into a sequence of ROS 2 actions.'}),"\n",(0,t.jsx)(e.li,{children:"Capstone Project: The Autonomous Humanoid. A final project where a simulated robot receives a voice command, plans a path, navigates obstacles, identifies an object using computer vision, and manipulates it."}),"\n"]})]})}function d(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(l,{...n})}):l(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>s,x:()=>c});var o=i(6540);const t={},a=o.createContext(t);function s(n){const e=o.useContext(a);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function c(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:s(n.components),o.createElement(a.Provider,{value:e},n.children)}}}]);